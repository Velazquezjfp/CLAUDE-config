#Lambda not in use in AWS, creating a version 3, but after comparing this and the one in production, 
#will get more notes in here to see what CLaude thinks, what is the main difference?
import json

def lambda_handler(event, context):
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }
# Hybrid version - works as both Lambda function and local script
# Based on bosenet_one_data_extraction_test_v2.py
from openpyxl import load_workbook
import datetime
from datetime import time, date
import json
import datefinder
from fuzzywuzzy import fuzz
import os
import base64
import io
import re

def find_date_column(workbook):
    sheet = workbook.active
    date_keywords = ["Datum", "Date"]
    date_row = None
    date_column = None
    date_found = False

    # Find the date column header
    for col in 'ABCDEF':
        for row in range(1, 101):
            cell_value = str(sheet[f'{col}{row}'].value or "")
            if any(keyword.lower() in cell_value.lower() for keyword in date_keywords):
                date_row = row
                date_column = col
                date_found = True
                break
        if date_found:
            break

    if not date_found:
        raise ValueError("No dates found on the file, ensure the file contains at least 2-3 values")

    confirmed = False
    date_list = []
    empty_count = 0
    row_iter = date_row + 1
    last_valid_date = None

    while empty_count <= 10 and row_iter <= sheet.max_row:
        cell = sheet[f'{date_column}{row_iter}']
        cell_value = str(cell.value if cell.value is not None else "")
        matches = list(datefinder.find_dates(cell_value))

        if matches:
            # Found a valid date
            date_obj = matches[0]
            
            # Extract digits from the original cell value
            import re
            digits = re.findall(r'\d+', cell_value)
            
            # If we have at least 3 digit groups (day, month, year)
            if len(digits) >= 3:
                # Always take first number as day, second as month, regardless of original format
                day = int(digits[0])
                month = int(digits[1])
                year = int(digits[2])
                
                # Make sure day/month are valid (swap if needed)
                if day > 31:  # Invalid day value, likely a year
                    if year <= 31:  # Year might be in day position
                        day, year = year, day
                if month > 12:  # Invalid month, swap with day if possible
                    if day <= 12:
                        day, month = month, day
                
                # Format as dd.mm.yyyy
                current_date = f"{day:02d}.{month:02d}.{year:04d}"
            else:
                # Fallback to date object if regex fails
                current_date = f"{date_obj.day:02d}.{date_obj.month:02d}.{date_obj.year}"
                
            date_list.append((row_iter, current_date))
            last_valid_date = current_date
            empty_count = 0
            if not confirmed and len(date_list) >= 2:
                confirmed = True
        elif cell_value.strip() == "":
            # Empty cell - use the last valid date if available
            if last_valid_date:
                date_list.append((row_iter, last_valid_date))
            empty_count += 1
        else:
            # Non-date, non-empty value - stop before this row
            if confirmed:
                print(f"Non-date value '{cell_value}' found at {date_column}{row_iter} after date sequence started. Stopping date collection.")
                break  # Stop collecting before including this non-date row
        
        row_iter += 1

    if not confirmed:
        raise ValueError("Date column lead could not be confirmed, please verify the data")

    return date_list, date_row, date_column

def fill_missing_dates(dates):
    filled_dates = []
    for i in range(len(dates)):
        row_num, date_val = dates[i]
        if i > 0:
            prev_row, prev_date = dates[i - 1]
            for missing in range(prev_row + 1, row_num):
                filled_dates.append((missing, prev_date))
        filled_dates.append((row_num, date_val))
    return filled_dates

def confirm_total_hours_column(sheet, col_num, found_row, categories):
    """
    Confirms if a total_hours match is valid by checking the next 2 rows
    for onsite/remote patterns. If onsite/remote found, this is a parent header.
    Returns True if confirmed as total_hours, False if parent header.
    """
    onsite_keywords = categories.get('onsite', [])
    remote_keywords = categories.get('remote', [])
    all_keywords = onsite_keywords + remote_keywords
    
    # Check next 2 rows in the same column
    for check_row in range(found_row + 1, found_row + 3):
        if check_row > sheet.max_row:
            break
            
        cell_value = str(sheet.cell(row=check_row, column=col_num).value or "")
        cell_value_clean = cell_value.replace('\n', ' ').replace('\r', ' ').strip()
        cell_value_clean = ' '.join(cell_value_clean.split())
        
        # Check if this matches onsite or remote keywords
        for keyword in all_keywords:
            ratio = fuzz.ratio(cell_value_clean.lower(), keyword.lower())
            if ratio > 90:
                print(f'DEBUG: Found onsite/remote pattern "{cell_value_clean}" in row {check_row} (ratio: {ratio})')
                return False  # This is a parent header, not a total_hours column
    
    # No onsite/remote patterns found in next 2 rows, confirm as total_hours
    return True

def find_column_for_category(sheet, start_cell_row, categories, exclude_columns):
    """
    Searches columns A to M for cells that match one of the category keywords.
    Returns a mapping of category names to column letters.
    """
    column_mappings = {}
    max_column = min(17, sheet.max_column)  # Limit search up to column M, just incremented just in case, originally was 12. 

    print(f"DEBUG: Looking for categories: {list(categories.keys())}")
    print(f"DEBUG: Starting search from row {start_cell_row}")

    for col in range(1, max_column + 1):
        column_letter = chr(64 + col)
        print(f'DEBUG: Checking column {column_letter}')
        if column_letter in exclude_columns:
            print(f'DEBUG: Skipping column {column_letter} (excluded)')
            continue
        found_match = False
        for row in range(start_cell_row, start_cell_row + 20):
            if row > sheet.max_row:
                break
            cell_value = str(sheet.cell(row=row, column=col).value)
            # Preprocess cell value: replace actual newlines with spaces and clean up
            cell_value_clean = cell_value.replace('\n', ' ').replace('\r', ' ').strip()
            # Remove extra spaces
            cell_value_clean = ' '.join(cell_value_clean.split())
            print(f'DEBUG: Col {column_letter} Row {row}: "{cell_value}" -> cleaned: "{cell_value_clean}"')
            
            for category, keywords in categories.items():
                if category in column_mappings:
                    continue  # Skip if this category is already matched
                
                # Check each keyword for this category
                for keyword in keywords:
                    ratio = fuzz.ratio(cell_value_clean.lower(), keyword.lower())
                    if ratio > 90:
                        # Special confirmation for total_hours to avoid parent header conflicts
                        if category == 'total_hours':
                            confirmed = confirm_total_hours_column(sheet, col, row, categories)
                            if confirmed:
                                print(f'DEBUG: MATCH CONFIRMED! Category "{category}" matched with keyword "{keyword}"')
                                print(f'DEBUG: Cell value cleaned: "{cell_value_clean}" vs Keyword: "{keyword}" (ratio: {ratio})')
                                column_mappings[category] = column_letter
                                found_match = True
                                break
                            else:
                                print(f'DEBUG: MATCH REJECTED! Category "{category}" appears to be parent header (onsite/remote found below)')
                                print(f'DEBUG: Cell value cleaned: "{cell_value_clean}" vs Keyword: "{keyword}" (ratio: {ratio})')
                                # Continue searching for actual onsite/remote columns
                        else:
                            print(f'DEBUG: MATCH FOUND! Category "{category}" matched with keyword "{keyword}"')
                            print(f'DEBUG: Cell value cleaned: "{cell_value_clean}" vs Keyword: "{keyword}" (ratio: {ratio})')
                            column_mappings[category] = column_letter
                            found_match = True
                            break
                    else:
                        # Only print for uhrzeit categories to reduce noise
                        if category in ['uhrzeit_von', 'uhrzeit_bis']:
                            print(f'DEBUG: No match - "{cell_value_clean}" vs "{keyword}" (ratio: {ratio})')
                
                if found_match:
                    break
            if found_match:
                break
        if found_match:
            exclude_columns.append(column_letter)

    print(f'DEBUG: Final column mappings: {column_mappings}')
    return column_mappings

def extract_data_from_column(sheet, column, dates_list):
    """
    Extracts data from the specified column for the provided row numbers.
    Returns a list of tuples (row number, cell value).
    """
    data_list = []
    for row_tuple in dates_list:
        row_num = row_tuple[0]
        cell_value = sheet[f'{column}{row_num}'].value
        # Skip formula tokens or None
        if cell_value is None:
            continue
        data_list.append((row_num, cell_value))
    print(data_list)
    return data_list

def extract_table_data(workbook, start_cell_row, dates_list):
    sheet = workbook.active
    categories = {
        'onsite': ['vor Ort', 'On site'],
        'remote': ['Remote', 'Remote**', 'homeoffice', 'HomeOffice', 'Home office'],
        'jira': ['JIRA', 'Jira', 'jira'],
        'milestone' : ["Meilenstein"],
        'descriptions': ["Beschreibung", "Meilenstein/Task/Beschreibung", "Durchgeführte Arbeiten","Durchgeführte Arbeiten (zwingend stets zu den beauftragten Auftragsmeilensteinen zugeordnet)", "Meilenstein/Task/Beschreibung/verwendete IBM Software"],
        'epic': ['EPIC'],
        'slt': ['SLT'],
        'total_hours': ["geleistete Stunden", "Aufwand in Std. 0,00 **)", "Aufwand (Stunden)*", "Aufwand in Std."],
        #new parameters from client
        'uhrzeit_von': ["von 00:00"],
        'uhrzeit_bis': ["bis 00:00"],
        'pause_zeit':["Pause in Std./Min. 0:00 *)", "Pause in Std./Min."],
        'einsatzort': ["einsatzort", "Einsatzort"],
        'rufbereitschaft': ["Rufbereitschaft", "rufbereitschaft"]

    }
    alternative_categories = {
        'jira': ['JIRA'],
        'milestone' : ["Meilenstein"],
        'descriptions': ["Beschreibung", "Meilenstein/Task/Beschreibung", "Durchgeführte Arbeiten","Durchgeführte Arbeiten (zwingend stets zu den beauftragten Auftragsmeilensteinen zugeordnet)", "Meilenstein/Task/Beschreibung/verwendete IBM Software"],
        'epic': ['EPIC'],
        'slt': ['SLT'],
        'hours' : ['Aufwand (Stunden)*']
    }

    used_columns = ['A']
    # Use a temporary variable to hold the mappings found in the first pass
    initial_column_mappings = find_column_for_category(sheet, start_cell_row, categories, used_columns)

    # Initialize the final_column_mappings with the initial result
    final_column_mappings = initial_column_mappings

    #Some columns could overlap, since the aufwand could be a header, so the strategy is to look in the mappings for the category onsite and remote, if they do not exist then try category aufwand with the same function but only one item.   
    print(f'this are the column mappings {initial_column_mappings}')
    print(f'this are the column mappings type {type(initial_column_mappings)}')

    # Check if 'remote' or 'onsite' are keys
    if 'remote' in initial_column_mappings or 'onsite' in initial_column_mappings:
        # Continue in the program
        print("Remote or onsite column is present. Continuing...")
    else:
        # DISABLED: Alternative categories fallback
        print("Neither remote nor onsite column found. Alternative categories lookup is DISABLED for testing.")
        # Do alternative instruction
        # print("Neither remote nor onsite column found, looking for aufwand category/column")
        # #defining columns again
        # used_columns_alt = ['A']
        # final_column_mappings = find_column_for_category(sheet, start_cell_row, alternative_categories, used_columns_alt) # Overwrite final_column_mappings
    
    # NEW REQUIREMENT 1: Extract unidentified columns between date and descriptions
    # Find date column and descriptions column positions
    date_column_pos = None
    descriptions_column_pos = None
    
    # Get date column from find_date_column function (we need to access it)
    dates_list_temp, date_row_temp, date_column = find_date_column(workbook)
    date_column_pos = ord(date_column) - ord('A') + 1  # Convert to number
    
    # Find descriptions column position
    descriptions_col = final_column_mappings.get('descriptions')
    if descriptions_col:
        descriptions_column_pos = ord(descriptions_col) - ord('A') + 1
    
    # Extract unidentified columns between date and descriptions
    unidentified_columns = {}
    if date_column_pos and descriptions_column_pos and descriptions_column_pos > date_column_pos:
        used_column_letters = list(final_column_mappings.values()) + [date_column]
        
        for col_num in range(date_column_pos + 1, descriptions_column_pos):
            col_letter = chr(64 + col_num)
            if col_letter not in used_column_letters:
                # Get column header name
                header_value = None
                for header_row in range(start_cell_row, start_cell_row + 20):
                    if header_row > sheet.max_row:
                        break
                    cell_value = sheet.cell(row=header_row, column=col_num).value
                    if cell_value and str(cell_value).strip():
                        header_value = str(cell_value).strip()
                        break
                
                if header_value:
                    # Store the column mapping first
                    unidentified_columns[header_value] = col_letter

    results = {}
    for category, column in final_column_mappings.items():
        data = extract_data_from_column(sheet, column, dates_list)
        if data:
            results[f'{category}_list'] = data
    
    # Add unidentified columns to results
    for header_name, col_letter in unidentified_columns.items():
        data = extract_data_from_column(sheet, col_letter, dates_list)
        if data:
            results[f'{header_name}_list'] = data
    
    return results, final_column_mappings, unidentified_columns

def process_headers(workbook, x_cell_limit):
    """
    Extracts master data header information from the file.
    Returns a dictionary with parameter names as keys.
    """
    sheet = workbook.active

    """
    Old parameters rolling out on september  3th 2025
    parameters = [
        "Auftraggeber","Verantwortlicher Projektleiter (sachlicher Zeichnender):", "Verantwortlicher Projektleiter", "Auftragnehmer", "Verantwortlicher AG","Verantwortlicher AN (Ansprechpartner der Firma):", "Verantwortlicher AN", 
        "Vorhaben", "Vertrags-Nr", "Projektbezeichnung", "Projekt", "Vertragsnummer BAMF", "Einsatzbericht Monat", "Leistungserbringer(in) (Subdienstleister)",
        "Leistungserbringer(in)", "Rolle im Projekt", "Dienstort", "Meilenstein", "Aktenzeichen BAMF", 
        "Bestellnummer", "VC-ID", "PG (Role im Projekt)", "PO"
    ]
    """

    parameters = [
        "Vertragsnummer/Kennung Auftraggeber",
        "Vertragsnummer/Kennung Auftragnehmer",
        "Vertragsnummer/Kennung Unterauftragnehmer",
        "Auftraggeber",
        "Verantwortlicher Projektleiter (sachlicher Zeichnender):",
        "Verantwortlicher Projektleiter",
        "Auftragnehmer",
        "Verantwortlicher AG",
        "Verantwortlicher AN (Ansprechpartner der Firma):",
        "Verantwortlicher AN",
        "Vorhaben",
        "Vertrags-Nr",
        "Projektbezeichnung",
        "Projekt",
        "Vertragsnummer BAMF",
        "Einsatzbericht Monat",
        "Leistungserbringer(in) (Subdienstleister)",
        "Leistungserbringer(in)",
        "Leistung erbracht durch",
        "Rolle im Projekt",
        "Dienstort",
        "Meilenstein",
        "Aktenzeichen BAMF",
        "Bestellnummer",
        "VC-ID",
        "PG (Role im Projekt)",
        "PO",
        "Leistungszeitraum",
        "Unterauftragnehmer"
    ]
    
    found_parameters = {}
    assigned_column = None
    remaining_parameters = set(parameters)

    for col in range(1, 12):  # Search columns A to K
        for row in range(1, x_cell_limit + 1):
            cell_value = sheet.cell(row=row, column=col).value
            if isinstance(cell_value, str):
                cell_value = cell_value.strip()
                # Check for exact match first
                if cell_value in remaining_parameters:
                    found_parameters[cell_value] = row
                    assigned_column = col
                    remaining_parameters.remove(cell_value)
                    continue

                # Otherwise check fuzzy or substring matches
                for param in list(remaining_parameters):
                    if fuzz.ratio(cell_value, param) >= 100 or param in cell_value:
                        found_parameters[param] = row
                        assigned_column = col
                        remaining_parameters.remove(param)
                        break
        
        if found_parameters:
            break

    extracted_data = {}
    if assigned_column:
        # Look in the columns following the one where parameters were found
        for col in range(assigned_column + 1, 12):
            values_found = False
            for param, row in found_parameters.items():
                value = sheet.cell(row=row, column=col).value
                if value:
                    extracted_data[param] = value
                    values_found = True
            if values_found:
                break

    # Ensure all parameters are present, setting missing ones to None
    for param in found_parameters.keys():
        extracted_data.setdefault(param, None)

    # Convert datetime objects to '%d.%m.%Y'
    for key, value in extracted_data.items():
        if isinstance(value, datetime.datetime):
            extracted_data[key] = value.strftime('%d.%m.%Y')

    return extracted_data

def convert_datetimes_to_strings(obj):
    """
    Recursively walk through a dict/list/tuple and convert any datetime objects into strings.
    """
    if isinstance(obj, dict):
        return {k: convert_datetimes_to_strings(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_datetimes_to_strings(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_datetimes_to_strings(item) for item in obj)
    elif isinstance(obj, datetime.datetime):
        return obj.strftime('%d.%m.%Y')
    elif isinstance(obj, datetime.time):
        return obj.strftime('%H:%M:%S')
    elif isinstance(obj, datetime.date):
        return obj.strftime('%d.%m.%Y')
    else:
        return obj

def safe_convert_to_number(value):
    """
    Safely convert a value to a number (int or float).
    Returns 0 if conversion fails.
    """
    if value is None:
        return 0
    
    # If it's already a number, return it
    if isinstance(value, (int, float)):
        return value
    
    # If it's a string, try to convert it
    if isinstance(value, str):
        # Remove common formatting characters
        cleaned_value = value.replace(',', '.').replace(' ', '').strip()
        
        try:
            # Try to convert to float first (handles decimals)
            if '.' in cleaned_value:
                return float(cleaned_value)
            else:
                return int(cleaned_value)
        except (ValueError, TypeError):
            # If conversion fails, return 0
            print(f"Warning: Could not convert '{value}' to number. Using 0 instead.")
            return 0
    
    # For any other type, return 0
    print(f"Warning: Unexpected type {type(value)} for value '{value}'. Using 0 instead.")
    return 0

def format_hours(value):
    """
    Format hours value: keep integers as int, round floats to 2 decimal places.
    """
    if value is None:
        return 0
    
    # Convert using existing function
    numeric_value = safe_convert_to_number(value)
    
    # If it's an integer or a float that equals an integer, return as int
    if isinstance(numeric_value, (int, float)) and numeric_value == int(numeric_value):
        return int(numeric_value)
    
    # If it's a float, round to 2 decimal places
    if isinstance(numeric_value, float):
        return round(numeric_value, 2)
    
    return numeric_value

def create_final_json(master_data, table_results, dates_list_fixed, final_column_mappings, unidentified_columns):
    keys_column_mappings = list(final_column_mappings.keys())
    # NEW: Add unidentified column names to the list
    unidentified_keys = list(unidentified_columns.keys())
    all_field_keys = keys_column_mappings + unidentified_keys
    
    category_data = {}
    for key, value_list in table_results.items():
        field = key.replace('_list', '')
        mapping = {row: val for row, val in value_list}
        category_data[field] = mapping

    table_details = []
    total_remote_hours = 0
    total_onsite_hours = 0
    total_hours = 0
    
    for row, date_val in dates_list_fixed:
        entry = {"id": row, "date": date_val}
        # Process all fields (identified + unidentified)
        for field in all_field_keys:
            entry[field] = category_data.get(field, {}).get(row, None)
            
            # For unidentified columns, ensure string values
            if field in unidentified_keys:
                if entry[field] is not None:
                    entry[field] = str(entry[field])
                else:
                    entry[field] = ""
            
            # Safe conversion and addition with exception handling
            try:
                if field == 'remote' and entry[field] is not None:
                    numeric_value = safe_convert_to_number(entry[field])
                    total_remote_hours += numeric_value
                    # Format the entry value
                    entry[field] = format_hours(entry[field])
                    
                elif field == 'onsite' and entry[field] is not None:
                    numeric_value = safe_convert_to_number(entry[field])
                    total_onsite_hours += numeric_value
                    # Format the entry value
                    entry[field] = format_hours(entry[field])
                    
                elif field == 'hours' and entry[field] is not None:
                    numeric_value = safe_convert_to_number(entry[field])
                    total_hours += numeric_value
                    # Format the entry value
                    entry[field] = format_hours(entry[field])
                    
                    # NEW RULE: Special case for hours with einsatzort (same logic as total_hours)
                    # Check einsatzort value from category_data for this row
                    einsatzort_raw = category_data.get('einsatzort', {}).get(row, None)
                    if einsatzort_raw is not None and str(einsatzort_raw).strip():
                        einsatzort_value = str(einsatzort_raw).strip().lower()
                        if einsatzort_value == 'remote':
                            total_remote_hours += numeric_value
                            print(f"DEBUG: Adding {numeric_value} hours from hours to total_remote_hours (einsatzort: remote)")
                        elif einsatzort_value == 'onsite':
                            total_onsite_hours += numeric_value
                            print(f"DEBUG: Adding {numeric_value} hours from hours to total_onsite_hours (einsatzort: onsite)")
                    
                elif field == 'total_hours' and entry[field] is not None:
                    numeric_value = safe_convert_to_number(entry[field])
                    total_hours += numeric_value
                    # Format the entry value
                    entry[field] = format_hours(entry[field])
                    
                    # NEW RULE: Special case for total_hours with einsatzort
                    # Check einsatzort value from category_data for this row
                    einsatzort_raw = category_data.get('einsatzort', {}).get(row, None)
                    if einsatzort_raw is not None and str(einsatzort_raw).strip():
                        einsatzort_value = str(einsatzort_raw).strip().lower()
                        if einsatzort_value == 'remote':
                            total_remote_hours += numeric_value
                            print(f"DEBUG: Adding {numeric_value} hours from total_hours to total_remote_hours (einsatzort: remote)")
                        elif einsatzort_value == 'onsite':
                            total_onsite_hours += numeric_value
                            print(f"DEBUG: Adding {numeric_value} hours from total_hours to total_onsite_hours (einsatzort: onsite)")
                    
            except Exception as e:
                print(f"Error processing field '{field}' with value '{entry[field]}' at row {row}: {str(e)}")
                # Continue processing other fields
                continue
        
        # Add standard fields to entry
        entry['ai_check'] = None
        entry['hours_check'] = []
        entry['warnings'] = None
                
        table_details.append(entry)

    # Only add totals if they're greater than 0, format hours
    if total_remote_hours > 0:
        master_data["total_remote_hours"] = format_hours(total_remote_hours)

    if total_onsite_hours > 0:
        master_data["total_onsite_hours"] = format_hours(total_onsite_hours)

    # Calculate total_hours for Masterdata:
    # Priority 1: Use accumulated total_hours (includes both classified and unclassified)
    # Priority 2: If no total_hours accumulated, fall back to remote+onsite sum
    if total_hours > 0:
        # Use accumulated total_hours (includes all records: classified + unclassified)
        master_data["total_hours"] = format_hours(total_hours)
        print(f"DEBUG: Using accumulated total_hours: {total_hours} (includes classified + unclassified)")
    elif total_remote_hours > 0 or total_onsite_hours > 0:
        # Fall back to remote+onsite sum when no total_hours field exists
        master_total_hours = total_remote_hours + total_onsite_hours
        master_data["total_hours"] = format_hours(master_total_hours)
        print(f"DEBUG: Using remote+onsite sum for total_hours: {total_remote_hours} + {total_onsite_hours} = {master_total_hours}")
    # If neither exists, don't add total_hours to masterdata

    final_output = {
        "Masterdata": master_data,
        "TableDetails": table_details
    }
    return final_output

def extract_project_totals_table(workbook):
    """
    Enhanced Project_Totals table extraction with header detection
    Returns a list of dictionaries representing the table data
    """
    try:
        # Look for a table named "Project_Totals" in defined names
        if 'Project_Totals' not in workbook.defined_names:
            print("Project_Totals table not found in workbook")
            return None
        
        named_range = workbook.defined_names['Project_Totals']
        project_totals = []
        
        for sheet_name, cell_address in named_range.destinations:
            sheet = workbook[sheet_name]
            
            # Parse the range to get start and end cells
            if ':' in cell_address:
                start_cell, end_cell = cell_address.split(':')
                # Handle cell addresses with $ signs
                start_cell = start_cell.replace('$', '')
                end_cell = end_cell.replace('$', '')
                start_col = ord(start_cell[0]) - ord('A') + 1
                start_row = int(start_cell[1:])
                end_col = ord(end_cell[0]) - ord('A') + 1  
                end_row = int(end_cell[1:])
            else:
                # Single cell, expand to reasonable table size
                cell_address = cell_address.replace('$', '')
                start_col = ord(cell_address[0]) - ord('A') + 1
                start_row = int(cell_address[1:])
                end_col = start_col + 3  # Assume 4 columns
                end_row = start_row + 10  # Assume up to 10 rows
            
            # Calculate actual number of columns with data
            actual_columns = 0
            for col in range(start_col, end_col + 1):
                has_data = False
                for row in range(start_row, min(start_row + 5, end_row + 1)):  # Check first few rows
                    cell = sheet.cell(row=row, column=col)
                    if cell.value is not None and str(cell.value).strip():
                        has_data = True
                        break
                if has_data:
                    actual_columns = col - start_col + 1
            
            if actual_columns == 0:
                print("No data found in project_totals table")
                return None
            
            # Adjust end_col to actual columns
            end_col = start_col + actual_columns - 1
            
            # Check first row to determine if it contains headers or data
            first_row_values = []
            for col in range(start_col, end_col + 1):
                cell = sheet.cell(row=start_row, column=col)
                first_row_values.append(cell.value)
            
            # Determine if first row contains headers or data
            has_headers = True
            has_numeric = False
            
            for value in first_row_values:
                if value is not None:
                    if isinstance(value, (int, float)):
                        has_numeric = True
                        has_headers = False
                    elif isinstance(value, str):
                        # Try to convert string to number to check if it's numeric
                        try:
                            float(value.replace(',', '.').strip())
                            has_numeric = True
                            has_headers = False
                        except (ValueError, AttributeError):
                            # It's a string that can't be converted to number
                            pass
            
            # Set up headers
            if has_headers:
                # Extract headers from first row
                headers = []
                for col in range(start_col, end_col + 1):
                    header_cell = sheet.cell(row=start_row, column=col)
                    header_value = str(header_cell.value) if header_cell.value else f"Column_{col}"
                    headers.append(header_value)
                data_start_row = start_row + 1
            else:
                # Use dynamic column headers for any number of columns
                headers = [f"Column_{i+1}" for i in range(actual_columns)]
                data_start_row = start_row
            
            # Extract data rows
            for row in range(data_start_row, end_row + 1):
                row_data = {}
                has_data = False
                
                for col_idx, col in enumerate(range(start_col, end_col + 1)):
                    cell = sheet.cell(row=row, column=col)
                    cell_value = cell.value
                    
                    if cell_value is not None:
                        has_data = True
                        # Ensure we extract values (not formulas) and convert numeric values to float
                        if isinstance(cell_value, (int, float)):
                            row_data[headers[col_idx]] = float(cell_value)
                        elif isinstance(cell_value, str):
                            # Try to convert string numbers to float
                            cleaned_value = cell_value.replace(',', '.').strip()
                            try:
                                # Try to convert to float first
                                row_data[headers[col_idx]] = float(cleaned_value) if cleaned_value else 0.0
                            except ValueError:
                                # If conversion fails, keep as string
                                row_data[headers[col_idx]] = str(cell_value)
                        else:
                            # For other types, try to convert or keep as string
                            try:
                                row_data[headers[col_idx]] = float(cell_value)
                            except (ValueError, TypeError):
                                row_data[headers[col_idx]] = str(cell_value)
                    else:
                        # Handle None values - default to empty string
                        row_data[headers[col_idx]] = ""
                
                if has_data:
                    project_totals.append(row_data)
                elif project_totals:
                    # Stop if we hit empty row after finding data
                    break
            
            break  # Only process first range found
        
        return project_totals if project_totals else None
        
    except Exception as e:
        print(f"Error extracting Project_Totals table: {e}")
        return None

def process_excel_file(file_path):
    """
    Process an Excel file locally (non-Lambda mode)
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"File not found: {file_path}")
    
    if not (file_path.endswith('.xlsx') or file_path.endswith('.xlsm')):
        raise ValueError("Wrong file format, please provide a .xlsm or .xlsx file")
    
    # Load the workbook directly from file path
    workbook = load_workbook(file_path, data_only=True)
    
    # Data extraction
    dates_list, date_row, date_column = find_date_column(workbook)
    dates_list_fixed = fill_missing_dates(dates_list)
    start_cell_row = date_row - 3
    table_results, final_column_mappings, unidentified_columns = extract_table_data(workbook, start_cell_row, dates_list_fixed)
    master_data = process_headers(workbook, date_row)
    
    # JSON creation
    final_json_dict = create_final_json(master_data, table_results, dates_list_fixed, final_column_mappings, unidentified_columns)
    
    # Checksum processing
    checksum_value = None
    try:
        if 'Checksum' in workbook.defined_names:
            print(f'there is a checksum cell detected')
            named_range = workbook.defined_names['Checksum']
            for sheet_name, cell_address in named_range.destinations:
                sheet = workbook[sheet_name]
                cell_obj = sheet[cell_address]
                if hasattr(cell_obj, 'value'):  # Single cell
                    checksum_value = cell_obj.value
                elif isinstance(cell_obj, tuple):  # Range of cells
                    first_cell = cell_obj[0][0] if cell_obj and cell_obj[0] else None
                    checksum_value = first_cell.value if first_cell else None
                else:
                    checksum_value = None
                print(f'this is the value for checksum {checksum_value}')
                break
    except Exception as e:
        print(f"Failed to retrieve 'Checksum' value: {e}")
    
    # Extract named cell totals and add to master_data
    named_totals = {
        'Totals': 'xls_Totals',
        'Totals_Onsite': 'xls_Totals_Onsite', 
        'Totals_Remote': 'xls_Totals_Remote'
    }
    
    for named_cell, masterdata_key in named_totals.items():
        try:
            if named_cell in workbook.defined_names:
                print(f'Named cell "{named_cell}" detected')
                named_range = workbook.defined_names[named_cell]
                for sheet_name, cell_address in named_range.destinations:
                    sheet = workbook[sheet_name]
                    cell_obj = sheet[cell_address]
                    cell_value = None
                    
                    if hasattr(cell_obj, 'value'):  # Single cell
                        cell_value = cell_obj.value
                    elif isinstance(cell_obj, tuple):  # Range of cells
                        first_cell = cell_obj[0][0] if cell_obj and cell_obj[0] else None
                        cell_value = first_cell.value if first_cell else None
                    
                    # Try to convert to number
                    if cell_value is not None:
                        try:
                            # Use existing safe_convert_to_number function
                            numeric_value = safe_convert_to_number(cell_value)
                            if numeric_value != 0 or (isinstance(cell_value, (int, float)) and cell_value == 0):
                                # Only add if conversion successful (0 is valid if original was 0)
                                final_json_dict['Masterdata'][masterdata_key] = numeric_value
                                print(f'Added {masterdata_key}: {numeric_value}')
                        except Exception as conv_e:
                            print(f"Failed to convert '{named_cell}' value '{cell_value}' to number: {conv_e}")
                    break
        except Exception as e:
            print(f"Failed to retrieve '{named_cell}' value: {e}")
    
    # Data conversion and final processing
    final_json_dict = convert_datetimes_to_strings(final_json_dict)
    
    if final_json_dict and checksum_value is not None:
        final_json_dict['Checksum'] = checksum_value
    
    # Check the last ID before cleaning
    last_table_detail_id_complete = None
    if 'TableDetails' in final_json_dict and final_json_dict['TableDetails']:
        if isinstance(final_json_dict['TableDetails'][-1], dict) and 'id' in final_json_dict['TableDetails'][-1]:
            last_table_detail_id_complete = final_json_dict['TableDetails'][-1]['id']
    
    # Clean empty entries
    if 'TableDetails' in final_json_dict:
        cleaned_table_details = []
        for item in final_json_dict['TableDetails']:
            excluded_keys = {'id', 'date', 'ai_check', 'hours_check', 'warnings'}
            all_other_values_are_none = all(
                value is None or value == [] or value == "" for key, value in item.items() 
                if key not in excluded_keys
            )
            if not all_other_values_are_none:
                cleaned_table_details.append(item)
        final_json_dict['TableDetails'] = cleaned_table_details
    
    # Add metadata
    last_column_mapping_value = None
    if final_column_mappings:
        last_column_mapping_value = list(final_column_mappings.values())[-1]
    
    last_table_detail_id_cleaned = None
    if 'TableDetails' in final_json_dict and final_json_dict['TableDetails']:
        if isinstance(final_json_dict['TableDetails'][-1], dict) and 'id' in final_json_dict['TableDetails'][-1]:
            last_table_detail_id_cleaned = final_json_dict['TableDetails'][-1]['id']
    
    first_table_detail_id = None
    if 'TableDetails' in final_json_dict and final_json_dict['TableDetails']:
        first_item = final_json_dict['TableDetails'][0]
        if isinstance(first_item, dict) and 'id' in first_item:
            first_table_detail_id = first_item['id']
    
    # Add metadata to response
    final_json_dict['last_data_column'] = last_column_mapping_value
    final_json_dict['first_entry_id'] = first_table_detail_id
    final_json_dict['last_entry_id_complete'] = last_table_detail_id_complete
    final_json_dict['last_entry_id_cleaned'] = last_table_detail_id_cleaned
    
    # Extract Project_Totals table if present
    project_totals_data = extract_project_totals_table(workbook)
    if project_totals_data:
        final_json_dict['project_totals'] = project_totals_data
    
    return final_json_dict

def lambda_handler(event, context):
    """
    AWS Lambda handler (for Lambda mode)
    """
    print('Entering the processing section')
    print(f'Event type: {type(event)}')
    print(f'Event keys: {list(event.keys()) if isinstance(event, dict) else "Not a dict"}')
    
    try:
        # Enhanced event parsing to handle different Lambda invocation methods
        data = None
        
        # Method 1: Direct invocation (event contains the payload directly)
        if isinstance(event, dict) and ('file' in event and 'file_name' in event):
            data = event
            print('Using direct event format')
        
        # Method 2: API Gateway format (payload in body)
        elif isinstance(event, dict) and 'body' in event:
            try:
                if isinstance(event['body'], str):
                    data = json.loads(event['body'])
                else:
                    data = event['body']
                print('Using API Gateway body format')
            except (json.JSONDecodeError, TypeError) as e:
                print(f'Failed to parse body: {e}')
        
        # Method 3: Lambda URL format (similar to API Gateway)
        elif isinstance(event, dict) and 'requestContext' in event:
            try:
                if 'body' in event and event['body']:
                    data = json.loads(event['body']) if isinstance(event['body'], str) else event['body']
                    print('Using Lambda URL format')
            except (json.JSONDecodeError, TypeError) as e:
                print(f'Failed to parse Lambda URL body: {e}')
        
        # Method 4: String event (legacy)
        elif isinstance(event, str):
            try:
                data = json.loads(event)
                print('Using string event format')
            except json.JSONDecodeError as e:
                print(f'Failed to parse string event: {e}')
        
        # If still no data, try the event directly as fallback
        if not data:
            data = event
            print('Using fallback - direct event')
        
        if not data:
            return {
                "statusCode": 400,
                "body": json.dumps({"error": "Could not parse event data", "event_type": str(type(event))})
            }

        # File validation
        file_content = data.get('file')
        file_name = data.get('file_name')

        if not file_content or not file_name:
            return {
                "statusCode": 400,
                "body": json.dumps({"message": "Missing file or file_name in request"})
            }

        if not (file_name.endswith('.xlsx') or file_name.endswith('.xlsm')):
            raise ValueError("Wrong file format, please provide a .xlsm or .xlsx file")

        # File processing with exception handling
        try:
            binary_data = base64.b64decode(file_content)
            excel_file = io.BytesIO(binary_data)
            workbook = load_workbook(excel_file, data_only=True)
        except Exception as e:
            return {
                "statusCode": 500,
                "body": json.dumps({"error": f"Failed to load Excel file: {str(e)}", "status": "failed"})
            }

        # Data extraction with exception handling
        try:
            dates_list, date_row, date_column = find_date_column(workbook)
            dates_list_fixed = fill_missing_dates(dates_list)
            start_cell_row = date_row - 3
            table_results, final_column_mappings, unidentified_columns = extract_table_data(workbook, start_cell_row, dates_list_fixed)
            master_data = process_headers(workbook, date_row)
        except Exception as e:
            raise ValueError(f"Failed to extract data from Excel file = {str(e)}")

        # JSON creation with exception handling
        try:
            final_json_dict = create_final_json(master_data, table_results, dates_list_fixed, final_column_mappings, unidentified_columns)
        except Exception as e:
            raise ValueError(f"Failed to create final JSON = {str(e)}")

        # Checksum processing with exception handling
        checksum_value = None
        try:
            if 'Checksum' in workbook.defined_names:
                print(f'there is a checksum cell detected')
                named_range = workbook.defined_names['Checksum']
                for sheet_name, cell_address in named_range.destinations:
                    sheet = workbook[sheet_name]
                    cell_obj = sheet[cell_address]
                    if hasattr(cell_obj, 'value'):  # Single cell
                        checksum_value = cell_obj.value
                    elif isinstance(cell_obj, tuple):  # Range of cells
                        first_cell = cell_obj[0][0] if cell_obj and cell_obj[0] else None
                        checksum_value = first_cell.value if first_cell else None
                    else:
                        checksum_value = None
                    print(f'this is the value for checksum {checksum_value}')
                    break
        except Exception as e:
            print(f"Failed to retrieve 'Checksum' value: {e}")

        # Extract named cell totals and add to master_data
        named_totals = {
            'Totals': 'xls_Totals',
            'Totals_Onsite': 'xls_Totals_Onsite', 
            'Totals_Remote': 'xls_Totals_Remote'
        }
        
        for named_cell, masterdata_key in named_totals.items():
            try:
                if named_cell in workbook.defined_names:
                    print(f'Named cell "{named_cell}" detected')
                    named_range = workbook.defined_names[named_cell]
                    for sheet_name, cell_address in named_range.destinations:
                        sheet = workbook[sheet_name]
                        cell_obj = sheet[cell_address]
                        cell_value = None
                        
                        if hasattr(cell_obj, 'value'):  # Single cell
                            cell_value = cell_obj.value
                        elif isinstance(cell_obj, tuple):  # Range of cells
                            first_cell = cell_obj[0][0] if cell_obj and cell_obj[0] else None
                            cell_value = first_cell.value if first_cell else None
                        
                        # Try to convert to number
                        if cell_value is not None:
                            try:
                                # Use existing safe_convert_to_number function
                                numeric_value = safe_convert_to_number(cell_value)
                                if numeric_value != 0 or (isinstance(cell_value, (int, float)) and cell_value == 0):
                                    # Only add if conversion successful (0 is valid if original was 0)
                                    final_json_dict['Masterdata'][masterdata_key] = numeric_value
                                    print(f'Added {masterdata_key}: {numeric_value}')
                            except Exception as conv_e:
                                print(f"Failed to convert '{named_cell}' value '{cell_value}' to number: {conv_e}")
                        break
            except Exception as e:
                print(f"Failed to retrieve '{named_cell}' value: {e}")

        # Data conversion and final processing with exception handling
        try:
            final_json_dict = convert_datetimes_to_strings(final_json_dict)

            if final_json_dict and checksum_value is not None:
                final_json_dict['Checksum'] = checksum_value

            # Check the last ID before cleaning, so I can know where to print the whole table and also any warnings regarding weekly or monthly time periods.
            last_table_detail_id_complete = None
            # Check if TableDetails exists and is not empty AFTER cleaning
            if 'TableDetails' in final_json_dict and final_json_dict['TableDetails']:
                # Access the last item in the potentially cleaned list and get its 'id'
                # Add a check to ensure the last item is a dict and has an 'id' key
                if isinstance(final_json_dict['TableDetails'][-1], dict) and 'id' in final_json_dict['TableDetails'][-1]:
                    last_table_detail_id_complete = final_json_dict['TableDetails'][-1]['id']

            # --- Start of the updated cleaning and adding parameters logic ---
            if 'TableDetails' in final_json_dict:
                cleaned_table_details = []
                for item in final_json_dict['TableDetails']:
                    # Check if all values are None, EXCLUDING 'id', 'date', 'ai_check', 'hours_check', 'warnings'
                    excluded_keys = {'id', 'date', 'ai_check', 'hours_check', 'warnings'}
                    all_other_values_are_none = all(
                        value is None or value == [] or value == "" for key, value in item.items() 
                        if key not in excluded_keys
                    )

                    if not all_other_values_are_none:
                        # Keep items that have meaningful data (preserving the warnings, ai_check, hours_check set earlier)
                        cleaned_table_details.append(item)

                final_json_dict['TableDetails'] = cleaned_table_details
            # --- End of the updated cleaning and adding parameters logic ---

            # --- Add the new requested values here ---
            last_column_mapping_value = None
            if final_column_mappings: # Check if the dictionary is not empty
                # Get the value associated with the last key inserted
                last_column_mapping_value = list(final_column_mappings.values())[-1]

            last_table_detail_id_cleaned = None
            # Check if TableDetails exists and is not empty AFTER cleaning
            if 'TableDetails' in final_json_dict and final_json_dict['TableDetails']:
                # Access the last item in the potentially cleaned list and get its 'id'
                # Add a check to ensure the last item is a dict and has an 'id' key
                if isinstance(final_json_dict['TableDetails'][-1], dict) and 'id' in final_json_dict['TableDetails'][-1]:
                    last_table_detail_id_cleaned = final_json_dict['TableDetails'][-1]['id']
            
            first_table_detail_id = None
            # Check if TableDetails exists and is not empty AFTER cleaning
            if 'TableDetails' in final_json_dict and final_json_dict['TableDetails']:
                first_item = final_json_dict['TableDetails'][0] # Access the first item
                # Check to ensure the first item is a dictionary and has an 'id' key
                if isinstance(first_item, dict) and 'id' in first_item:
                    first_table_detail_id = first_item['id']

            # Add these values to the json response. I need them in order to print later on. 
            final_json_dict['last_data_column'] = last_column_mapping_value
            final_json_dict['first_entry_id'] = first_table_detail_id
            final_json_dict['last_entry_id_complete'] = last_table_detail_id_complete
            final_json_dict['last_entry_id_cleaned'] = last_table_detail_id_cleaned
            
            # NEW REQUIREMENT 3: Extract Project_Totals table if present
            project_totals_data = extract_project_totals_table(workbook)
            if project_totals_data:
                final_json_dict['project_totals'] = project_totals_data
            # --- End of adding new requested values ---

        except Exception as e:
            raise ValueError(f"Failed to process final data = {str(e)}")

        return {
            "statusCode": 200,
            "body": json.dumps(final_json_dict)
        }

    except Exception as e:
        return {
            "statusCode": 500,
            "body": json.dumps({"error": f"Error encountered while extracting data from timesheet results on existing Timesheet = {str(e)}", "status": "failed"})
        }

# LOCAL EXECUTION MODE
if __name__ == "__main__":
    import argparse
    import sys
    
    parser = argparse.ArgumentParser(description='Process Excel files for timesheet data extraction')
    parser.add_argument('file_path', help='Path to the Excel file (.xlsx or .xlsm)')
    parser.add_argument('-o', '--output', help='Output JSON file path (optional)')
    parser.add_argument('--pretty', action='store_true', help='Pretty print JSON output')
    
    args = parser.parse_args()
    
    try:
        print(f"Processing file: {args.file_path}")
        result = process_excel_file(args.file_path)
        
        # Format output
        if args.pretty:
            json_output = json.dumps(result, indent=2, ensure_ascii=False)
        else:
            json_output = json.dumps(result, ensure_ascii=False)
        
        # Output to file or stdout
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(json_output)
            print(f"Results written to: {args.output}")
        else:
            print("\n" + "="*50)
            print("EXTRACTION RESULTS:")
            print("="*50)
            print(json_output)
        
        print(f"\nProcessing completed successfully!")
        
    except Exception as e:
        print(f"Error: {str(e)}", file=sys.stderr)
        sys.exit(1)