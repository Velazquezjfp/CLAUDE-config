#App Endpoint
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, ValidationError, validator
from typing import List, Union
import torch
from transformers import BertTokenizer, BertForSequenceClassification
import numpy as np
import json
import spacy
from langdetect import detect
import prompt_expert
from nlp_explain import provide_rejection_reasons
import re
import logging
import time
from datetime import datetime
from db_manager import db, get_batch_id

app = FastAPI()

# Load spaCy models
nlp_en = spacy.load("en_core_web_lg")
nlp_de = spacy.load("de_core_news_lg")

# Setup logging
logging.basicConfig(filename='logs.txt', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')

class Sentence(BaseModel):
    sentence: str
    llm: bool = True
    test: bool = False

    @validator('sentence')
    def sentence_must_not_be_empty(cls, v):
        if not v or v.strip() == '':
            raise ValueError('Sentence cannot be empty or null')
        return v

class BatchSentences(BaseModel):
    sentences: List[str]
    llm: bool = True
    test: bool = False

# Load your model and tokenizer here to keep them ready
model_path = 'bert_classifier'
tokenizer = BertTokenizer.from_pretrained(model_path)
model = BertForSequenceClassification.from_pretrained(model_path)
model.eval()
device = torch.device("cpu")
model.to(device)

@app.post("/predict")
async def predict(request: Union[Sentence, BatchSentences]):
    try:
        # Validate batch sentences if needed
        if isinstance(request, BatchSentences):
            if not request.sentences:
                raise HTTPException(status_code=400, detail="Empty sentences list not allowed")
        
        # Handle both single and batch requests
        if isinstance(request, BatchSentences):
            # Batch processing with database logging
            batch_id = get_batch_id()
            results = []
            batch_data = []
            
            for sentence in request.sentences:
                start_time = time.time()
                result = process_single_sentence(sentence, request.llm, request.test)
                processing_time = int((time.time() - start_time) * 1000)  # Convert to milliseconds
                
                # Prepare data for batch database logging
                batch_data.append({
                    'sentence': sentence,
                    'answer': result.get('answer', ''),
                    'reason': result.get('reason', ''),
                    'mode': result.get('mode', ''),
                    'score': result.get('score'),
                    'processing_time_ms': processing_time,
                    'entropy_value': getattr(result, '_entropy', None),
                    'name_detected': getattr(result, '_name_detected', False),
                    'detected_name': getattr(result, '_detected_name', None),
                    'deep_thinking': getattr(result, '_deep_thinking', False),
                    'language': getattr(result, '_language', None)
                })
                results.append(result)

            # Log batch to database (skip if test mode)
            if not request.test:
                try:
                    db.log_batch_inferences(batch_data, batch_id)
                except Exception as db_error:
                    logging.error(f"Database logging error: {db_error}")

            return results
        else:
            # Single sentence processing with database logging
            start_time = time.time()
            result = process_single_sentence(request.sentence, request.llm, request.test)
            processing_time = int((time.time() - start_time) * 1000)

            # Log single inference to database (skip if test mode)
            if not request.test:
                try:
                    db.log_inference(
                        sentence=request.sentence,
                        answer=result.get('answer', ''),
                        reason=result.get('reason', ''),
                        mode=result.get('mode', ''),
                        score=result.get('score'),
                        processing_time_ms=processing_time,
                        entropy_value=getattr(result, '_entropy', None),
                        name_detected=getattr(result, '_name_detected', False),
                        detected_name=getattr(result, '_detected_name', None),
                        deep_thinking=getattr(result, '_deep_thinking', False),
                        language=getattr(result, '_language', None)
                    )
                except Exception as db_error:
                    logging.error(f"Database logging error: {db_error}")

            return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
    
def extract_content_within_braces(input_string):
    # Regular expression to find content within curly braces
    pattern = r'\{[^{}]*\}'
    # Search for the pattern in the string
    match = re.search(pattern, input_string)
    if match:
        return match.group(0)  # Return the matched content including braces
    return None  # Return None if no match is found

def detect_person_name(text):
    try:
        lang = detect(text)
        if lang == 'en':
            nlp = nlp_en
        elif lang == 'de':
            nlp = nlp_de
        else:
            return {'name_detected': False, 'name': None, 'confidence': None}
    except:
        print('could not detect language')
        return {'name_detected': False, 'name': None, 'confidence': None}
    
    doc = nlp(text)
    person_entities = [ent for ent in doc.ents if ent.label_ in ('PER', 'PERSON')]
    if person_entities:
        return {'name_detected': True, 'name': person_entities[0].text, 'confidence': None}
    else:
        return {'name_detected': False, 'name': None, 'confidence': None}

def predict_prob(sentence, tokenizer, model, device):
    inputs = tokenizer.encode_plus(
        sentence, 
        None, 
        add_special_tokens=True, 
        max_length=128, 
        padding='max_length', 
        return_attention_mask=True, 
        truncation=True,
        return_tensors='pt'
    )
    ids = inputs['input_ids'].to(device)
    mask = inputs['attention_mask'].to(device)
    with torch.no_grad():
        outputs = model(ids, attention_mask=mask)
        predictions = torch.softmax(outputs.logits, dim=1)
        predicted_class_id = torch.argmax(predictions).item()
        probabilities = predictions.cpu().numpy().flatten()
    return predicted_class_id, probabilities

def process_single_sentence(test_sentence: str, llm_enabled: bool = True, test_mode: bool = False):
    """Process a single sentence and return the analysis result"""
    result_analysis = ''
    
    # Detect language for additional metadata
    try:
        detected_language = detect(test_sentence)
    except:
        detected_language = None
    
    # Get prediction probabilities and class
    predicted_class, probabilities = predict_prob(test_sentence, tokenizer, model, device)
    if predicted_class == 0:
        result_analysis = 'rejected'
        reasons_rejection = provide_rejection_reasons(test_sentence)
    else:
        result_analysis = 'accepted'
        reasons_rejection = {'Rejection Reasons': []}

    ent = entropy(probabilities)
    bert_score = float(max(probabilities))

    # Detect names
    name_info = detect_person_name(test_sentence)

    deep_thinking = False
    if ent < 0.1 and min(probabilities) < 0.1:
        # Highly certain decision
        deep_thinking = False
    else:
        deep_thinking = True
        print('thinking deeper...')

    # Check if LLM is enabled and deep thinking is needed
    if deep_thinking and llm_enabled:
        # Use LLM (slow mechanism)
        think = prompt_expert.prompt_llm(test_sentence)
        raw_text = think.candidates[0].content.parts[0].text

        try:
            # Remove the ```json\n and ``` wrappers
            if raw_text.startswith('```json') and raw_text.endswith('```'):
                json_str = raw_text[len('```json\n'):-3].strip()
                parsed_json = json.loads(json_str)
                # Add mode parameter for LLM response
                parsed_json['mode'] = 'LLM'
                
                # Add metadata for database logging (stored as private attributes)
                parsed_json['_entropy'] = float(ent)
                parsed_json['_name_detected'] = name_info['name_detected']
                parsed_json['_detected_name'] = name_info.get('name')
                parsed_json['_deep_thinking'] = deep_thinking
                parsed_json['_language'] = detected_language
                
                logging.info(f"Description: {test_sentence}; result: {json_str}")
                return parsed_json
            else:
                raise ValueError("Unexpected format: no ```json``` wrapping.")
        except:
            result = {
                "answer": "rejected",
                "reason": "Bei der Verarbeitung dieser AktivitÃ¤t ist ein Fehler aufgetreten, und sie konnte nicht analysiert werden.",
                "mode": "LLM"
            }
            # Add metadata for database logging
            result['_entropy'] = float(ent)
            result['_name_detected'] = name_info['name_detected']
            result['_detected_name'] = name_info.get('name')
            result['_deep_thinking'] = deep_thinking
            result['_language'] = detected_language
            return result
    else:
        # Use BERT (fast mechanism)
        if name_info['name_detected']:
            result_analysis = 'rejected'
            reasons_rejection['Rejection Reasons'].append(f"Name in der Beschreibung erkannt, Namen sind nicht erlaubt: {name_info['name']}")
        
        # Handle uncertainty case when LLM is disabled
        if deep_thinking and not llm_enabled:
            result_analysis = 'rejected'
            note = "Bitte Ã¼berprÃ¼fen Sie diese Beschreibung erneut, das System war sich unsicher"
            if reasons_rejection['Rejection Reasons']:
                reason = "; ".join(reasons_rejection['Rejection Reasons']) + f"; {note}"
            else:
                reason = note
        else:
            reason = "; ".join(reasons_rejection['Rejection Reasons']) if reasons_rejection['Rejection Reasons'] else ""
        
        result = {
            "answer": result_analysis,
            "reason": reason,
            "mode": "BERT",
            "score": bert_score
        }
        
        # Add metadata for database logging (stored as private attributes)
        result['_entropy'] = float(ent)
        result['_name_detected'] = name_info['name_detected']
        result['_detected_name'] = name_info.get('name')
        result['_deep_thinking'] = deep_thinking
        result['_language'] = detected_language
        
        return result

def entropy(probabilities):
    return -np.sum(probabilities * np.log2(probabilities + 1e-9))

# Additional API endpoints for database access
@app.get("/stats/recent")
async def get_recent_stats(limit: int = 100):
    """Get recent inference results for monitoring"""
    try:
        results = db.get_recent_results(limit)
        return {"results": results, "count": len(results)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")

@app.get("/stats/daily")
async def get_daily_stats(days: int = 30):
    """Get daily statistics for the specified number of days"""
    try:
        stats = db.get_daily_stats(days)
        return {"daily_stats": stats}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")

@app.get("/stats/performance")
async def get_performance_stats():
    """Get model performance statistics"""
    try:
        performance = db.get_model_performance()
        return {"performance": performance}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")

@app.post("/export/training-data")
async def export_training_data(start_date: str = None, end_date: str = None, 
                              answer_filter: str = None, filename: str = "training_export.json"):
    """Export data for model training/fine-tuning"""
    try:
        filepath = db.export_to_json(
            filename=filename,
            start_date=start_date,
            end_date=end_date,
            answer_filter=answer_filter
        )
        return {
            "message": "Training data exported successfully",
            "filepath": filepath,
            "filters": {
                "start_date": start_date,
                "end_date": end_date,
                "answer_filter": answer_filter
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Export error: {str(e)}")

@app.get("/health")
async def health_check():
    """Health check endpoint including database status"""
    try:
        # Test database connection
        recent = db.get_recent_results(1)
        db_status = "connected"
    except Exception as e:
        db_status = f"error: {str(e)}"

    return {
        "status": "healthy",
        "database": db_status,
        "model": "loaded",
        "timestamp": datetime.now().isoformat()
    }

@app.post("/admin/reset-database")
async def reset_database(request: dict):
    """
    Reset database by deleting all records from inference_results table.
    Requires safety code "270195" in request body.
    """
    safety_code = request.get("safety_code")

    if not safety_code:
        raise HTTPException(status_code=403, detail="Invalid or missing safety code")

    if safety_code != "270195":
        raise HTTPException(status_code=403, detail="Invalid or missing safety code")

    try:
        deleted_count = db.reset_database(safety_code)
        return {
            "message": "Database reset successful",
            "deleted_records": deleted_count
        }
    except ValueError:
        raise HTTPException(status_code=403, detail="Invalid or missing safety code")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")
