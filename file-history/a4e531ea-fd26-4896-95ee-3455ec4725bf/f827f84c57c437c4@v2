"""
AI Insights Generator for Dashboard
Uses Gemini Flash 2.0 to analyze inference patterns and provide actionable insights
"""
import os
import sqlite3
import json
import google.generativeai as genai
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configuration constants
BATCH_SIZE = 500

class AIInsightsGenerator:
    def __init__(self, db_path: str):
        self.db_path = db_path
        
        # Initialize Gemini API
        api_key = os.getenv('GOOGLE_API_KEY')
        if not api_key:
            raise ValueError("GOOGLE_API_KEY not found in environment variables")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-2.0-flash')
        
        # Initialize insights database
        self.init_insights_table()
    
    def init_insights_table(self):
        """Create insights table if it doesn't exist"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS ai_insights (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    insight_type TEXT NOT NULL,
                    content TEXT NOT NULL,
                    metadata TEXT,
                    expires_at DATETIME
                )
            ''')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_insights_type ON ai_insights(insight_type)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_insights_expires ON ai_insights(expires_at)')
    
    def get_cached_insight(self, insight_type: str) -> Optional[Dict]:
        """Get cached insight if still valid"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute('''
                SELECT * FROM ai_insights 
                WHERE insight_type = ? AND expires_at > datetime('now')
                ORDER BY timestamp DESC LIMIT 1
            ''', (insight_type,))
            
            row = cursor.fetchone()
            if row:
                return {
                    'content': row['content'],
                    'timestamp': row['timestamp'],
                    'metadata': json.loads(row['metadata']) if row['metadata'] else {}
                }
        return None
    
    def store_insight(self, insight_type: str, content: str, metadata: Dict = None, hours_valid: int = 3):
        """Store insight in database with expiration"""
        expires_at = datetime.now() + timedelta(hours=hours_valid)
        metadata_json = json.dumps(metadata) if metadata else None

        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                INSERT INTO ai_insights (insight_type, content, metadata, expires_at)
                VALUES (?, ?, ?, ?)
            ''', (insight_type, content, metadata_json, expires_at))

    def _fetch_records_in_batches(self, batch_size: int = BATCH_SIZE):
        """Generator that yields batches of records from database"""
        offset = 0

        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row

            while True:
                cursor = conn.execute('''
                    SELECT * FROM inference_results
                    ORDER BY timestamp DESC
                    LIMIT ? OFFSET ?
                ''', (batch_size, offset))

                batch = [dict(row) for row in cursor.fetchall()]

                if not batch:
                    break

                yield batch
                offset += batch_size

    def _process_batch_summary(self, batch_data: List[Dict]) -> str:
        """Generate condensed summary for a single batch of records"""
        if not batch_data:
            return "No data in batch"

        # Calculate batch statistics
        total = len(batch_data)
        accepted = sum(1 for r in batch_data if r['answer'] == 'accepted')
        rejected = total - accepted
        bert_usage = sum(1 for r in batch_data if r['mode'] == 'BERT')
        llm_usage = total - bert_usage

        # Get average scores for BERT
        bert_scores = [r['score'] for r in batch_data if r['score'] is not None and r['mode'] == 'BERT']
        avg_bert_score = sum(bert_scores) / len(bert_scores) if bert_scores else 0

        # Sample rejection reasons (top 3)
        rejection_reasons = [r['reason'] for r in batch_data if r['answer'] == 'rejected' and r['reason']]
        from collections import Counter
        common_reasons = Counter(rejection_reasons).most_common(3)

        # Create condensed prompt
        prompt = f"""
Summarize this batch of {total} activity classifications in 2-3 sentences:
- Accepted: {accepted}, Rejected: {rejected} (rate: {round(accepted/total*100, 1)}%)
- BERT: {bert_usage}, LLM: {llm_usage}
- Avg BERT score: {round(avg_bert_score, 3)}
- Top rejections: {', '.join([f'{reason} ({count}x)' for reason, count in common_reasons[:3]])}

Focus on key patterns and notable trends only.
"""

        try:
            response = self.model.generate_content(prompt)
            return response.candidates[0].content.parts[0].text.strip()
        except Exception as e:
            return f"Batch summary error: {str(e)[:50]}"

    def _generate_final_verdict(self, batch_summaries: List[str]) -> str:
        """Generate final consolidated analysis from all batch summaries"""
        if not batch_summaries:
            return "No batch summaries available"

        # Create consolidated prompt
        summaries_text = "\n\n".join([f"Batch {i+1}: {summary}" for i, summary in enumerate(batch_summaries)])

        prompt = f"""
Analyze these {len(batch_summaries)} batch summaries and provide 3-4 actionable insights in English:

{summaries_text}

Provide insights in this format:
1. [Overall pattern observation across all batches]
2. [Usage trend analysis - BERT vs LLM efficiency]
3. [Quality assessment and recommendations]

Keep insights brief (1-2 sentences each), actionable, and in English. Focus on what the compliance team should know.
"""

        try:
            response = self.model.generate_content(prompt)
            return response.candidates[0].content.parts[0].text.strip()
        except Exception as e:
            return f"Final verdict error: {str(e)[:100]}"

    def sample_recent_data(self, limit: int = 100) -> Dict[str, Any]:
        """Intelligently sample recent inference data"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            
            # Get recent data
            recent_cursor = conn.execute('''
                SELECT * FROM inference_results 
                ORDER BY timestamp DESC LIMIT ?
            ''', (limit,))
            recent_data = [dict(row) for row in recent_cursor.fetchall()]
            
            if not recent_data:
                return {'summary': 'No data available'}
            
            # Calculate summary statistics
            total = len(recent_data)
            accepted = sum(1 for r in recent_data if r['answer'] == 'accepted')
            rejected = total - accepted
            bert_usage = sum(1 for r in recent_data if r['mode'] == 'BERT')
            llm_usage = total - bert_usage
            
            # Get average scores for BERT
            bert_scores = [r['score'] for r in recent_data if r['score'] is not None and r['mode'] == 'BERT']
            avg_bert_score = sum(bert_scores) / len(bert_scores) if bert_scores else 0
            
            # Sample rejection reasons (top 5 most common)
            rejection_reasons = [r['reason'] for r in recent_data if r['answer'] == 'rejected' and r['reason']]
            from collections import Counter
            common_reasons = Counter(rejection_reasons).most_common(5)
            
            # Get time range
            time_range = f"{recent_data[-1]['timestamp']} to {recent_data[0]['timestamp']}"
            
            return {
                'summary': {
                    'total_inferences': total,
                    'accepted': accepted,
                    'rejected': rejected,
                    'acceptance_rate': round(accepted/total*100, 1),
                    'bert_usage': bert_usage,
                    'llm_usage': llm_usage,
                    'avg_bert_score': round(avg_bert_score, 3),
                    'time_range': time_range
                },
                'common_rejections': common_reasons[:3],  # Top 3 for brevity
                'sample_entries': recent_data[:10]  # First 10 for pattern analysis
            }
    
    def generate_pattern_analysis(self) -> str:
        """Generate insights about classification patterns using batch processing"""
        # Check cache first
        cached = self.get_cached_insight('pattern_analysis')
        if cached:
            return cached['content']

        # Process records in batches
        batch_summaries = []
        total_records = 0

        try:
            for batch in self._fetch_records_in_batches():
                total_records += len(batch)
                batch_summary = self._process_batch_summary(batch)
                batch_summaries.append(batch_summary)

            # Check if we have data
            if not batch_summaries:
                insight = "Insufficient data available for pattern analysis."
                self.store_insight('pattern_analysis', insight)
                return insight

            # Generate final verdict from all batch summaries
            insight_content = self._generate_final_verdict(batch_summaries)

            # Store in cache
            metadata = {
                'data_points': total_records,
                'batch_count': len(batch_summaries)
            }
            self.store_insight('pattern_analysis', insight_content, metadata, hours_valid=3)

            return insight_content

        except Exception as e:
            error_insight = f"AI analysis error: {str(e)[:100]}..."
            self.store_insight('pattern_analysis', error_insight, {'error': True}, hours_valid=1)
            return error_insight
    
    def generate_quality_assessment(self) -> str:
        """Generate quality assessment of recent classifications using batch processing"""
        # Check cache
        cached = self.get_cached_insight('quality_assessment')
        if cached:
            return cached['content']

        # Process records in batches for quality metrics
        batch_summaries = []
        total_records = 0
        total_accepted = 0
        total_bert = 0
        all_bert_scores = []

        try:
            for batch in self._fetch_records_in_batches():
                total_records += len(batch)
                total_accepted += sum(1 for r in batch if r['answer'] == 'accepted')
                total_bert += sum(1 for r in batch if r['mode'] == 'BERT')

                # Collect BERT scores
                bert_scores = [r['score'] for r in batch if r['score'] is not None and r['mode'] == 'BERT']
                all_bert_scores.extend(bert_scores)

                # Generate quality-focused batch summary
                batch_summary = self._process_batch_summary(batch)
                batch_summaries.append(batch_summary)

            # Check if we have data
            if not batch_summaries or total_records == 0:
                insight = "Insufficient data available for quality assessment."
                self.store_insight('quality_assessment', insight)
                return insight

            # Calculate overall quality metrics
            avg_bert_score = sum(all_bert_scores) / len(all_bert_scores) if all_bert_scores else 0
            uncertain_cases = total_records - total_bert
            acceptance_rate = round(total_accepted / total_records * 100, 1)
            confidence_level = "high" if avg_bert_score > 0.85 else "medium" if avg_bert_score > 0.7 else "low"

            # Create quality-focused prompt for final verdict
            summaries_text = "\n\n".join([f"Batch {i+1}: {summary}" for i, summary in enumerate(batch_summaries)])

            prompt = f"""
Assess the quality of AI classifications based on {len(batch_summaries)} batch summaries and overall metrics:

OVERALL QUALITY INDICATORS:
- Total Classifications: {total_records}
- BERT Confidence Score: {round(avg_bert_score, 3)} (average)
- Uncertain Cases (requiring LLM): {uncertain_cases} out of {total_records}
- Rejection Rate: {100-acceptance_rate}%

BATCH SUMMARIES:
{summaries_text}

Provide a brief quality assessment (2-3 sentences) in English:
- Is the classification quality good/medium/poor?
- Are there too many uncertain cases?
- What recommendations for improvement?
"""

            response = self.model.generate_content(prompt)
            insight_content = response.candidates[0].content.parts[0].text.strip()

            metadata = {
                'avg_score': round(avg_bert_score, 3),
                'uncertain_cases': uncertain_cases,
                'confidence_level': confidence_level,
                'batch_count': len(batch_summaries),
                'data_points': total_records
            }
            self.store_insight('quality_assessment', insight_content, metadata, hours_valid=4)

            return insight_content

        except Exception as e:
            error_insight = f"Quality assessment unavailable: {str(e)[:50]}..."
            self.store_insight('quality_assessment', error_insight, {'error': True}, hours_valid=1)
            return error_insight
    
    def get_all_insights(self) -> Dict[str, str]:
        """Get all current insights"""
        insights = {}
        
        # Generate insights (cached if available)
        insights['pattern_analysis'] = self.generate_pattern_analysis()
        insights['quality_assessment'] = self.generate_quality_assessment()
        
        return insights
    
    def cleanup_expired_insights(self):
        """Remove expired insights from database"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("DELETE FROM ai_insights WHERE expires_at < datetime('now')")


# Example usage and testing
if __name__ == "__main__":
    # Test the insights generator
    db_path = "../shared/data/inference_results.db"
    
    if os.path.exists(db_path):
        generator = AIInsightsGenerator(db_path)
        
        print("🧠 Testing AI Insights Generator...")
        print("=" * 50)
        
        insights = generator.get_all_insights()
        
        for insight_type, content in insights.items():
            print(f"\n📊 {insight_type.replace('_', ' ').title()}:")
            print("-" * 30)
            print(content)
        
        print("\n✅ AI Insights test completed!")
    else:
        print("❌ Database not found. Please run the main stack first.")