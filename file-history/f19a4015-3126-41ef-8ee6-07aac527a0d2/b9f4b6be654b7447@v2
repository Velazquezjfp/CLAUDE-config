"""
Test Case: TC-F-003-06
Requirement: F-003 - Implement batch processing with internal summaries for AI insights
Description: Test with 5000 records, verify process completes without token limit errors
Generated: 2025-10-01T14:59:00Z
"""

import sqlite3
import time

def test_TC_F_003_06():
    """Verify large dataset (5000 records) processes without token limit errors"""

    db_path = "/home/javiervel/clients/bosenet/ai-timesheet/inference/docker_files_api_copy2/shared/data/inference_results.db"

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    try:
        # Clear database first
        cursor.execute("DELETE FROM inference_results")
        cursor.execute("DELETE FROM ai_insights WHERE insight_type = 'pattern_analysis'")
        conn.commit()

        print("Inserting 5000 test records (this may take a moment)...")

        # Insert 5000 records (will be processed in 10 batches of 500)
        for i in range(5000):
            cursor.execute("""
                INSERT INTO inference_results
                (sentence, answer, reason, mode, score, timestamp, batch_id)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                f"TC-F-003-06 large dataset record {i}",
                "accepted" if i % 3 != 0 else "rejected",
                "Test reason" if i % 3 == 0 else "",
                "LLM" if i % 10 == 0 else "BERT",
                0.80 + (i % 15) * 0.01,
                time.time(),
                None
            ))

            # Commit in chunks for performance
            if i % 500 == 0 and i > 0:
                conn.commit()
                print(f"  Inserted {i} records...")

        conn.commit()
        print("✓ 5000 records inserted successfully")

        # Import AIInsightsGenerator
        import sys
        sys.path.insert(0, '/home/javiervel/clients/bosenet/ai-timesheet/inference/docker_files_api_copy2/dashboard/src')
        from ai_insights import AIInsightsGenerator

        generator = AIInsightsGenerator(db_path)

        # Mock Gemini API to count batches and avoid real API calls
        batch_count = 0

        class MockResponse:
            def __init__(self, text):
                self.text = text
                self.candidates = [self]
                self.content = self
                self.parts = [self]

        def mock_generate_content(prompt):
            nonlocal batch_count
            batch_count += 1
            return MockResponse(f"Mock summary for large dataset batch {batch_count}")

        from unittest.mock import patch

        print("Processing 5000 records in batches...")
        try:
            with patch.object(generator.model, 'generate_content', side_effect=mock_generate_content):
                verdict = generator.generate_pattern_analysis()

            # Should process 10 batches (500 each) + 1 final verdict = 11 API calls
            assert batch_count == 11, f"Expected 11 API calls (10 batches + 1 final), got {batch_count}"
            assert len(verdict) > 0, "Verdict should not be empty"

            print(f"✓ Successfully processed 5000 records in {batch_count-1} batches")
            print(f"✓ No token limit errors encountered")
            print(f"✓ Total API calls: {batch_count} (10 batch summaries + 1 final verdict)")

        except Exception as e:
            if "token" in str(e).lower() or "limit" in str(e).lower():
                raise AssertionError(f"Token limit error occurred: {e}")
            raise

    finally:
        cursor.execute("DELETE FROM inference_results WHERE sentence LIKE 'TC-F-003-06%'")
        conn.commit()
        conn.close()

if __name__ == "__main__":
    try:
        test_TC_F_003_06()
        print("TC-F-003-06: PASSED")
    except AssertionError as e:
        print(f"TC-F-003-06: FAILED - {e}")
    except Exception as e:
        print(f"TC-F-003-06: ERROR - {e}")
